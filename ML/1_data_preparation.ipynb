{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34e75a64",
   "metadata": {},
   "source": [
    "# Data Preparation - Viral Social Media Trends Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d20e55",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'visionHunter (Python 3.12.12)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n visionHunter ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204e267d",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "RAW_DATA_PATH = \"data/raw/Cleaned_Viral_Social_Media_Trends.csv\"\n",
    "PROCESSED_DIR = Path(\"data/processed\")\n",
    "ARTIFACTS_DIR = Path(\"artifacts\")\n",
    "FIGURES_DIR = Path(\"artifacts/figures\")\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [PROCESSED_DIR, ARTIFACTS_DIR, FIGURES_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    # Clear all files in directory\n",
    "    for file in dir_path.glob('*'):\n",
    "        if file.is_file():\n",
    "            file.unlink()\n",
    "\n",
    "# Constants\n",
    "# TODO : Implement needed constraints\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bcf2c2",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc672ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iqr_bounds(series, multiplier=1.5):\n",
    "    \"\"\"Calculate IQR-based outlier bounds\"\"\"\n",
    "    q1, q3 = series.quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    return q1 - multiplier * iqr, q3 + multiplier * iqr\n",
    "\n",
    "# TODO : Implement other Functions here \n",
    "\n",
    "def build_preprocessor(nominal_cols, ordinal_cols, ordinal_categories, numeric_cols):\n",
    "    \"\"\"Build preprocessing pipeline - Steps 6 & 7\"\"\"\n",
    "    transformers = [\n",
    "        ('num', Pipeline([\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_cols),\n",
    "        ('nom', Pipeline([\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', min_frequency=0.01, sparse_output=False))\n",
    "        ]), nominal_cols)\n",
    "    ]\n",
    "    \n",
    "    if ordinal_cols:\n",
    "        transformers.append(('ord', Pipeline([\n",
    "            ('ordinal', OrdinalEncoder(categories=ordinal_categories, handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "        ]), ordinal_cols))\n",
    "    \n",
    "    return ColumnTransformer(transformers)\n",
    "\n",
    "def extract_feature_names(fitted_ct):\n",
    "    \"\"\"Extract feature names from fitted ColumnTransformer\"\"\"\n",
    "    names = []\n",
    "    for name, transformer, columns in fitted_ct.transformers_:\n",
    "        if name == 'num':\n",
    "            names.extend(columns)\n",
    "        elif name == 'nom':\n",
    "            names.extend(transformer.named_steps['onehot'].get_feature_names_out(columns))\n",
    "        elif name == 'ord':\n",
    "            names.extend(columns)\n",
    "    return names\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce38eca",
   "metadata": {},
   "source": [
    "## Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e328f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(RAW_DATA_PATH)\n",
    "print(f\"‚úÖ Loaded {len(df_raw):,} rows, {len(df_raw.columns)} columns\")\n",
    "\n",
    "# Drop unnecessary text columns immediately\n",
    "df_raw = df_raw.drop(columns=['Post_ID'], errors='ignore')\n",
    "print(f\"‚úÖ Dropped host_name and name columns\")\n",
    "\n",
    "print(f\"\\nColumns: {list(df_raw.columns)}\")\n",
    "df_raw.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d000ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value statistics\n",
    "missing = df_raw.isnull().sum()\n",
    "missing_pct = (missing / len(df_raw) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing.index,\n",
    "    'Missing_Count': missing.values,\n",
    "    'Missing_Pct': missing_pct.values\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MISSING VALUE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(missing_df[missing_df['Missing_Count'] > 0].to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(df_raw.isnull(), cbar=False, yticklabels=False, cmap='viridis', ax=ax1)\n",
    "ax1.set_title('Missing Value Pattern (Yellow = Missing)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "missing_nonzero = missing_df[missing_df['Missing_Count'] > 0]\n",
    "if len(missing_nonzero) > 0:\n",
    "    ax2.barh(missing_nonzero['Column'], missing_nonzero['Missing_Pct'], color='coral', edgecolor='black')\n",
    "    ax2.set_xlabel('Missing %', fontsize=12)\n",
    "    ax2.set_title('Missing Value Percentage by Column', fontsize=14, fontweight='bold')\n",
    "    ax2.invert_yaxis()\n",
    "    for i, (col, pct) in enumerate(zip(missing_nonzero['Column'], missing_nonzero['Missing_Pct'])):\n",
    "        ax2.text(pct + 0.5, i, f'{pct:.1f}%', va='center', fontsize=10)\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No Missing Values', ha='center', va='center', fontsize=16, transform=ax2.transAxes)\n",
    "    ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'missing_values_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Missing value analysis saved to {FIGURES_DIR / 'missing_values_analysis.png'}\")\n",
    "print(f\"\\nüìù Strategy: Impute after split (Step 3) using TRAIN statistics only\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e595c92",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e988fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Summary:\")\n",
    "print(df_raw.describe())\n",
    "print(f\"\\n‚úÖ EDA complete - Ready for STEP 2: Split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a15258",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f4231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.describe(include='object')\n",
    "for col in df_raw.select_dtypes(include='object').columns:\n",
    "    print(f\"\\n{col}: {df_raw[col].nunique()} unique values\")\n",
    "    print(df_raw[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e31c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "numeric_cols = df_raw.select_dtypes(include=['float64', 'int64']).columns\n",
    "correlation_matrix = df_raw[numeric_cols].corr()\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(\n",
    "            correlation_matrix, \n",
    "            annot=True,\n",
    "            cmap='coolwarm',\n",
    "            center=0,\n",
    "            fmt='.2f',\n",
    "            square=True,\n",
    "            linewidths=0.5\n",
    "            )\n",
    "\n",
    "plt.title('Correlation Matrix of Numeric Features', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Correlation analysis saved to {FIGURES_DIR / 'correlation_matrix.png'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b479b60",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b30425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for distribution\n",
    "for i in df_raw.select_dtypes(include=\"number\").columns:\n",
    "    sns.histplot(data=df_raw, x=i, kde=True, color=\"red\")\n",
    "    plt.title(f'Distribution of {i}')\n",
    "    plt.savefig(FIGURES_DIR / f'Histogram-distribution of {i}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.tight_layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341fd31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df_raw['Platform'].value_counts()\n",
    "colors = [\"#FF0000\", \"#69C9D0\", \"#E1306C\", \"#1DA1F2\"]\n",
    "plt.pie(count, labels=count.index, colors=colors ,autopct='%1.1f%%')\n",
    "plt.title('Platform Distribution', fontsize=15 , weight='bold')\n",
    "plt.savefig(FIGURES_DIR / f'PieChart.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_distribution = df_raw['Region'].value_counts()\n",
    "colors = [ \"#001f54\" ,\"#d62828\",\"#3a86ff\",\"#f4d35e\",\"#264653\",\"#ffe066\",\"#ff99ac\",\"#cc5803\",\"#001f54\" ,\"#d62828\" ]\n",
    "plt.pie(region_distribution, labels=region_distribution.index, autopct='%1.1f%%', colors=colors)\n",
    "plt.title('Region Distribution', fontsize=15 , weight='bold')\n",
    "plt.savefig(FIGURES_DIR / f'Region Piechart.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be16ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "sns.barplot(x='Platform', y='Views',data=df_raw, palette='viridis',ci=False)\n",
    "plt.title('Total Views by Platform', fontsize=15 , weight='bold')\n",
    "plt.xlabel('Platform', weight='bold')\n",
    "plt.ylabel('Total Views', weight='bold')\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))\n",
    "plt.savefig(FIGURES_DIR / f'Barchart-Platform.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff9b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_views = df_raw.groupby('Content_Type')['Views'].sum().reset_index()\n",
    "sns.barplot(x='Content_Type', y='Views', data=content_views, palette='Set2')\n",
    "plt.title('Total Views by Content Type', fontsize=15 , weight='bold')\n",
    "plt.xlabel('Content Type', weight='bold')\n",
    "plt.ylabel('Total Views', weight='bold')\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))\n",
    "plt.savefig(FIGURES_DIR / f'Barchart-ContentType.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91010b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "views_per_country = df_raw.groupby('Region')['Views'].sum()\n",
    "sns.barplot(x=views_per_country.index, y=views_per_country.values , palette='Reds')\n",
    "plt.title('Total Views per Country', fontsize=15 , weight='bold')\n",
    "plt.xlabel('Country', weight='bold')\n",
    "plt.ylabel('Total Views', weight='bold')\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{int(x):,}'))\n",
    "plt.savefig(FIGURES_DIR / f'Barchart-Country.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7793f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "most5_used_hastags = df_raw['Hashtag'].value_counts().sort_values(ascending=False).nlargest(5)\n",
    "sns.barplot(x=most5_used_hastags.index, y=most5_used_hastags.values, palette='Greens')\n",
    "plt.title('Top 5 Most Used Hashtags', fontsize=15 , weight='bold')\n",
    "plt.xlabel('Hashtag', weight='bold')\n",
    "plt.ylabel('Frequency', weight='bold')\n",
    "plt.savefig(FIGURES_DIR / f'Barchart-Hashtags.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dc0bd3",
   "metadata": {},
   "source": [
    "## Split Train/Test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4883b9",
   "metadata": {},
   "source": [
    "#### Define feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc41bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af747a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOMINAL_COLS = ['Platform', 'Hashtag', 'Region', 'Content_Type']\n",
    "ORDINAL_COLS = ['Engagement_level']\n",
    "ORDINAL_CATEGORIES = [['High', 'Medium', 'Low']]\n",
    "NUMERIC_COLS = ['Views', 'Likes', 'Shares', 'Comments']\n",
    "\n",
    "\n",
    "feature_cols = NOMINAL_COLS + ORDINAL_COLS +  NUMERIC_COLS\n",
    "targets = NUMERIC_COLS\n",
    "target_cal = ORDINAL_COLS\n",
    "\n",
    "X = df_raw[feature_cols].copy()\n",
    "y_reg = df_raw[targets].copy()\n",
    "y_cal = df_raw[target_cal].copy()\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train_cal, X_test_cal, y_train_cal, y_test_cal = train_test_split(\n",
    "    X, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Step 2 complete:\")\n",
    "print(f\"For predicting continous values:\")\n",
    "print(f\"   TRAIN: {len(X_train_reg):,} samples\")\n",
    "print(f\"   TEST:  {len(X_test_reg):,} samples\")\n",
    "print(f\"\\n‚ö†Ô∏è NO FITTING HAS OCCURRED YET - ALL STATISTICS WILL USE TRAIN ONLY\")\n",
    "\n",
    "print(f\"For classificaion:\")\n",
    "print(f\"   TRAIN: {len(X_train_cal):,} samples\")\n",
    "print(f\"   TEST:  {len(X_test_cal):,} samples\")\n",
    "print(f\"\\n‚ö†Ô∏è NO FITTING HAS OCCURRED YET - ALL STATISTICS WILL USE TRAIN ONLY\")\n",
    "\n",
    "# TODO : Continue other Steps after this Split and do other"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visionHunter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
